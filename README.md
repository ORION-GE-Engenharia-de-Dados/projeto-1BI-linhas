# üìä (GE) Engenharia de Dados - Pipeline ETL com 1 Bilh√£o de Linhas

## üéØ Objetivo

Este projeto tem como objetivo desenvolver um pipeline completo de Engenharia de Dados, processando um dataset com 1 bilh√£o de linhas. O pipeline seguir√° todas as etapas de um processo profissional de ETL (Extra√ß√£o, Transforma√ß√£o e Carga), utilizando ferramentas **open source** amplamente utilizadas no mercado.

---

## üìå Contexto dos Dados

O dataset ser√° gerado sinteticamente. O tema pode ser escolhido entre os seguintes exemplos:

- **Financeiro:** Transa√ß√µes de cart√µes de cr√©dito
- **E-commerce:** Hist√≥rico de compras de usu√°rios
- **Mobilidade:** Corridas de aplicativos de transporte
- **IoT:** Leituras de sensores em tempo real

O formato final dos dados ser√° `.csv` ou `.parquet`.

---

## üõ†Ô∏è Ferramentas do Pipeline

| Etapa | Ferramenta | Descri√ß√£o |
|-----|------|----|
| Gera√ß√£o de Dados | Python (Faker, Pandas, Polars) | Cria√ß√£o de dados sint√©ticos |
| Armazenamento | MinIO | Data Lake para os arquivos brutos |
| Explora√ß√£o Local | DuckDB | An√°lises explorat√≥rias e valida√ß√£o |
| Consulta Distribu√≠da | Trino (Presto) | Query engine distribu√≠do conectado ao MinIO |
| Transforma√ß√µes | DBT | Modelagem de dados com camadas staging, intermediate e mart |
| Orquestra√ß√£o | Apache Airflow | Agendamento e execu√ß√£o dos pipelines |
| (Opcional) Visualiza√ß√£o | Apache Superset ou Metabase | Cria√ß√£o de dashboards |

---

## üìç Fases do Projeto

### Fase 1 ‚Äì Gera√ß√£o de Dados

- Desenvolver um script em Python para gerar um arquivo com 1 bilh√£o de linhas.
- Exemplos de colunas (tema financeiro):
  - `transaction_id`
  - `customer_id`
  - `transaction_amount`
  - `transaction_date`
  - `merchant_category`
  - `payment_method`
- Formato de sa√≠da: `.csv` ou `.parquet`.

---

### Fase 2 ‚Äì Upload para o MinIO

- Subir os arquivos para o bucket `landing-zone` no MinIO.
- Configurar o MinIO localmente via Docker.

---

### Fase 3 ‚Äì Explora√ß√£o de Dados com DuckDB

- Conectar ao arquivo local ou ao MinIO.
- Realizar an√°lises b√°sicas:
  - Contagem de linhas
  - Valida√ß√£o de tipos
  - Estat√≠sticas descritivas

---

### Fase 4 ‚Äì Query Distribu√≠da com Trino

- Instalar e configurar o Trino (ou Presto).
- Criar cat√°logo apontando para o MinIO.
- Executar queries SQL distribu√≠das.

Exemplos de queries:
- `SELECT COUNT(*)`
- Agrega√ß√µes por categoria
- Filtragem por intervalo de datas

---

### Fase 5 ‚Äì Transforma√ß√µes com DBT

- Estruturar o projeto DBT.
- Criar as seguintes camadas:

| Camada | Finalidade |
|----|----|
| Staging | Tratamento inicial e padroniza√ß√£o de dados |
| Intermediate | Limpeza, enriquecimento e jun√ß√µes |
| Mart | Tabelas finais anal√≠ticas |

- Executar os modelos DBT com Trino como backend.

---

### Fase 6 ‚Äì Orquestra√ß√£o com Airflow

- Criar um DAG no Airflow com as seguintes tasks:

1. **Gera√ß√£o de dados**
2. **Upload para MinIO**
3. **Valida√ß√£o com DuckDB**
4. **Consultas com Trino**
5. **Execu√ß√£o de modelos DBT**
6. **(Opcional) Gera√ß√£o de dashboard**

---

## ‚úÖ Tecnologias

- **Python**
- **MinIO**
- **DuckDB**
- **Trino (Presto)**
- **DBT**
- **Apache Airflow**
- **(Opcional) Apache Superset ou Metabase**

---

## üí¨ Observa√ß√µes

- Todas as ferramentas utilizadas devem ser **open source** e de uso livre.
- A complexidade e o n√≠vel de detalhamento s√£o graduais: o foco √© o aprendizado.
- A escolha do contexto dos dados fica livre para o grupo.

---

**Bons estudos! üöÄ**
